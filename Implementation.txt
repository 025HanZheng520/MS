# Description and Implementation
1. ##Data Preparation
   - **Description**: 
    DFEW：It is the biggest and most accessible to the public for dynamic facial video frame datasets. The dataset comprises 16,732 video clips extracted from 1500 high-definition movies. Each clip is annotated by professional annotators with seven basic emotions, namely happiness, sadness, neutral, anger, surprise, disgust, and fear. A fivefold crossover experimental evaluation is
executed on 12,509 carefully selected valid video clips.
    FERV38K:The dataset contains 38,935 video clips from 22 different scenes, covering a variety of real-life scenarios such as lectures, schools, social situations, and daily life. Experts have meticulously added seven basic emotion labels to this large-scale dynamic facial expression dataset. The dataset was divided into a training set comprising 80% and a test set consisting of the remaining 20%.
   - **Data Download
    These two datasets are large open-source datasets. To support the originality and legitimacy of the datasets, you must visit the official website to apply for permission to use them.
2. ##Environment Setup
  - The environment required for this method can be found in the requirements.txt file.
  - The specific dependencies required for this method are listed in dependency.txt. To install all dependencies at once, please use the following command:
    pip install -r dependency.txt 
3. ## Model Training
  -Step 1: Install dependencies as outlined above.
  -Step 2: Ensure that the dataset is processed into a .txt format file for input compatibility.
  -Step 3: The model is stored in the model folder. This method primarily utilizes ResNet18 to create a two-layer pyramid structure, followed by using CRF (Conditional Random Fields) to enhance the model’s interactions. Finally, a top-down approach is used to fuse multi-scale features.
  -Step 4: Train the model using the command line with the following command:
    python main.py
    This method follows a training/validation mode and evaluates the model using WAR (Weighted Accuracy Rate) and UAR (Unweighted Accuracy Rate).
  -Step 5: The training logs are saved in the log folder.

